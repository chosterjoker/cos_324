{"cells":[{"cell_type":"markdown","metadata":{"id":"5F62B_4pcRI-"},"source":["# Programming Assignment 5: Value Iteration\n"]},{"cell_type":"markdown","source":["## Collaboration statement\n","\n","List all collaborators by name, including other students and/or course staff (e.g. TAs, UCAs). If you collaborated with no one on this assignment, write the following: ``\"I did not collaborate with anyone on this assignment.\"``\n","\n","**TODO: I did not collaborate with anyone on this assignment.**"],"metadata":{"id":"LoKkTi4Gy7FF"}},{"cell_type":"markdown","metadata":{"id":"052JVvHvcbQ9"},"source":["## Problem Setting\n","\n","**Gridworld**: In this assignment, we use the popular RL toolkit  [OpenAI Gym](https://gym.openai.com/)  to implement _value iteration_ on a gridworld environment (Lecture 18). In the gridworld environment we consider, an agent needs to navigate on a 2-D plane by using 4 actions (left, right, up, and down). It starts at a designated state (starting point \"S\"), and needs to try and reach a goal state (\"G\"). The agent controls the movement of a character in a grid world. Some tiles on the grid are walkable (\"S\", \"G\", and \"F\"), and others (\"H\") lead to the agent falling into the water, which ends the episode. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The agent stays in the same position if the transition would otherwise have put them out of bounds.\n","\n","**Rewards**: The environment is such that reaching the goal state gives the agent a `+1` reward, and it gets a `0` reward in other states (detailed description below). Hence, reaching the goal state is equivalent to maximizing the rewards it can accumulate.\n","\n","**Uncertainty**: The environment also has some uncertainty. That is, the movement direction of the agent is uncertain and only partially depends on the chosen direction. In our setting (detailed below), each tile in the grid is \"slippery\", which corresponds to the following uncertainty; if the agent chose to move up/down, it will be able to do so with probability `1/3`, but with probability `2/3` it will move right/left (`1/3` right, `1/3` left). Similarly for the case that the agent chose to move right/left, it might move up/down instead. However, you will always move by 1 tile only (i.e., no \"jumps\")."]},{"cell_type":"markdown","metadata":{"id":"t19XeaxoQJiI"},"source":["## Environment: Frozen Lake\n","Winter is here. You and your friends are tossing around a frisbee at the park when you make a wild throw that leaves the frisbee stranded out in the middle of\n","a lake. The water is mostly frozen (\"F\"), but there are a few holes (\"H\") where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend, and sometimes slip into another, adjacent tile! The surface is described using a grid like the following $8 \\times 8$ example:\n","\n","    SFFFFFFF\n","    FFFFFFFF\n","    FFFHFFFF\n","    FFFFFHFF\n","    FFFHFFFF\n","    FHHFFFHF\n","    FHFFHFHF\n","    FFFHFFFG\n","\n","\n","    S : starting point, safe\n","    F : frozen surface, safe\n","    H : hole, fall to your doom\n","    G : goal, where the frisbee is located\n","\n","\n","The episode ends when you reach the goal or fall in a hole \"H\". You receive a reward of 1 if you reach the goal, and 0 otherwise. The steps that you can make are one of the following: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3.\n","\n","## Visualization\n","It is easier to understand how this environment behaves if you see it first.  \n","\n","**HINT:** We **strongly recommend** that before implementing anything, you start by first running all cells of this notebook, and scroll down to the cell under **ACT7** see how a **totally-random** agent moves around the frozen lake.\n","\n","You will see it fail many times. In what follows, we will implement a much better agent that can navigate to the goal. **You can use the visualization to see how your agent performs, where it fails, and debug issues that arise during development.**\n","      "]},{"cell_type":"markdown","metadata":{"id":"JCQ3b6BQQJiI"},"source":["## Notation\n","\n","Mapping of the OpenAI gym constructs to the notation used in lectures:\n","* The value function $V(s)$, is implemented here as a small array, of size corrsponsing to number of states, such that `V[s]` contains the value $V(s)$.\n","* The transition probabilities of the environment $P(s'|s,a)$ correspond to the gym object `env.P` described below.\n","* In lectures, the reward function $r(a|s, s')$ signified  taking action $a$ while in state $s$ moving to state $s'$. However, in our setting here the reward fixed into the environment is such that $r(a|s=\\text{'G'}, s')=1$ and $r(a|s \\neq\\text{'G'}, s')=0$ for any $a, s'$. In other words, in this setting we only get the reward if we *actually reach* the goal state."]},{"cell_type":"markdown","metadata":{"id":"odNaDE1zyrL2"},"source":["### First, install dependencies (takes ~1 min)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8-AxnvAVyzQQ","executionInfo":{"status":"ok","timestamp":1733101769133,"user_tz":300,"elapsed":20473,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay pygame > /dev/null 2>&1\n","!apt-get install -y xvfb libav-tools python-opengl ffmpeg > /dev/null 2>&1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pdb2JwZy4jGj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733101770516,"user_tz":300,"elapsed":1386,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}},"outputId":"73ee0f60-87d5-4bf1-f458-b5b14a7b1f48"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import resource_stream, resource_exists\n","/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n","/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n","/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n"]}],"source":["import gym\n","import numpy as np\n","import random\n","from IPython.display import clear_output\n","import time\n","import pygame\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython import display"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogyVgSS0Qlx_","executionInfo":{"status":"ok","timestamp":1733101770516,"user_tz":300,"elapsed":4,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}},"outputId":"b5e90fc0-b13d-4e6e-ba4b-e5f624b945b2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["<Surface(640x480x32 SW)>"]},"metadata":{},"execution_count":3}],"source":["# setting up dummy display driver for Colab\n","import os\n","os.environ['SDL_VIDEODRIVER']='dummy'\n","pygame.display.set_mode((640,480))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dGEFMfDOzLen","executionInfo":{"status":"ok","timestamp":1733101770750,"user_tz":300,"elapsed":237,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# first, we set some useful parameters:\n","gamma = 0.99        # discount factor\n","theta = 0.000001    # precision of value_iteration\n","\n","# let's set up the frozen lake environment.\n","env = gym.make(\"FrozenLake8x8-v1\", new_step_api=True) # create the environment\n","env = env.unwrapped # unwrap it to have additional information from it\n","\n","# spaces dimension\n","num_actions = env.action_space.n # we can move up/down/left/right, i.e., 4 actions.\n","num_states = env.observation_space.n # we have one state per tile in the grid (64 states)\n","\n","# NOTE, the transition probabilities we mentioned above (i.e., the \"slipperiness\"), are already given\n","# in the env.P object. Specifically, env.P are the transition probabilities (dictionary dict of dicts of lists)\n","#           P[s][a] == [(probability, s', reward, done),\n","#                       (probability, s', reward, done),\n","#                       (probability, s', reward, done), ]\n","\n","# As an example, for state s=0 (the starting tile \"S\"),and action a=0 (LEFT), you will have:\n","#           P[0][0] == [(1/3, 0, 0, False), (1/3, 0, 0, False), (1/3, 4, 0, False)]\n","# since:\n","# with probability 1/3 you'll move UP (but there's no up, so you'll stay put at s=0),\n","# with probability 1/3 you'll move LEFT (but there's no left, so you'll stay put at s=0),\n","# with probability 1/3 you'll move DOWN (so you'll get to the tile below, which is s'=8).\n"]},{"cell_type":"markdown","metadata":{"id":"gDP6Ff27k_uk"},"source":["## Value-Iteration algorithm\n","\n","We will now implement the Value Iteration algorithm you have seen in class. The pseudo-code is given below."]},{"cell_type":"markdown","metadata":{"id":"oPLXehxpQJiL"},"source":["### Pseudo-code\n","1. Parameter: a small threshold $\\theta>0$ determining accuracy of estimation.\n","1. Initialize $V(s)=-\\frac{M}{1-\\gamma}$, for all $s\\in\\mathbb{S}$, where $M$ is the upper bound on the absolute value of immediate rewards.\n","1. **Loop**:\n","    1. $\\Delta \\leftarrow 0$.\n","    1. **Loop for each** $s\\in\\mathbb{S}$:\n","        1. $v\\leftarrow V(s)$.\n","        1. $V(s) \\leftarrow \\max_a\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr]$.\n","        1. $\\Delta\\leftarrow \\max(\\Delta, \\lvert v - V(s) \\rvert)$.\n","1. **until** $\\Delta < \\theta$.\n","1. Output a deterministic policy, $\\pi\\approx\\pi_*$, such that\n","$$ \\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr].$$"]},{"cell_type":"markdown","metadata":{"id":"TZrrQafMQJiL"},"source":["## ACTS 1-2: Computing the sum & argmax of Value-Iteration.\n","\n","Compute the inner sum $$\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr],$$ given $V, s, a, \\gamma$."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5topu9WrLNo_","executionInfo":{"status":"ok","timestamp":1733101770750,"user_tz":300,"elapsed":2,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# ACT 1: compute the inner sum of the equations above, given:\n","        # Value function V (an array of size num_states),\n","        # State s, Action a, and discount factor gamma.\n","        # Arguments (V, s, a, gamma) should not be mutated in this ACT\n","\n","def sum_over_states(V, s, a, gamma):\n","    sum_val = 0\n","    # recall that env.P is a list of tuples for (s,a): [(prob, s', r, done),...] (See block above for more details!)\n","    # where 'prob' is the transition probability P(s'|s,a).\n","    # also, since r is associated with the states, you can think of the sum as only traversing states s'.\n","    # therefore, all is needed is to iterate over each possible transition to state s'\n","    # from the pair (s,a), and compute the sum value (as in the pseudo-code).\n","    ### YOUR CODE GOES HERE\n","    for probability, state_after, reward, done in env.P[s][a]:\n","      sum_val += probability * (reward + gamma * V[state_after])\n","\n","    return sum_val\n"]},{"cell_type":"markdown","metadata":{"id":"pzRc5WrWQJiM"},"source":["We now compute the argmax operation, in which we will update $\\pi(a|s)$ where $s$ is the given state, and $a$ is the maximizing action! To obtain it, you should use the function above `sum_over_states`.\n","\n","**HINT**:\n","In the scaffold code for ACT5-6, pi is initialized to a zero matrix. If all the ACTs are implemented correctly, then the optimal policy pi returned by the `value_iteration` function (from ACT5-6) should have the following property: each row in pi should contain all zeros except at one index, where it contains a 1 (i.e. the index corresponding to the maximizing action for the state corresponding to that row).\n","\n","Suppose we had a simple environment with 2 states and 3 actions. Then, our pi would be a 2 by 3 matrix. Suppose the following pi matrix was returned by `value_iteration`:\n","\n","$$\n","\\pi =\n","\\begin{bmatrix}\n","  1 & 0 & 0 \\\\\n","  0 & 0 & 1 \\\\\n","\\end{bmatrix}\n","$$\n","\n","This would mean that when we're in state 0 (i.e. row index 0), we should take action 0 (i.e. column index 0 contains a 1) and when we're in state 1 (i.e. row index 1), we should take action 2 (i.e. column index 2 contains a 1).\n","\n","Now going back to `argmax_over_actions` in ACT2, Given state s, suppose action a maximizes the inner sum. How should pi be updated? (You can assume pi[s] contains all zeros, from the scaffold code in ACT5-6)."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"b7WLWpUMQJiM","executionInfo":{"status":"ok","timestamp":1733101770750,"user_tz":300,"elapsed":2,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# ACT 2: update the action which can take us to a higher valued state, given:\n","        # Value function V (an array of size num_states),\n","        # Policy array pi (a matrix of shape [num_states, num_actions], you can assume it contains all zero entries),\n","        # State s, and discount factor gamma.\n","        # Return the updated pi matrix and max_val (i.e. the value of taking the maximizing action from state s)\n","\n","def argmax_over_actions(V, pi, s, gamma):\n","    max_val = 0\n","    ### YOUR CODE GOES HERE\n","    optimal_action = None\n","\n","    for action in range(num_actions):\n","      current_value = sum_over_states(V, s, action, gamma)\n","\n","      if current_value > max_val:\n","          max_val = current_value\n","          optimal_action = action\n","\n","    pi[s] = [0] * len(env.P[s])\n","    pi[s][optimal_action] = 1\n","\n","    return pi, max_val"]},{"cell_type":"markdown","metadata":{"id":"3LXHKhMPQJiM"},"source":["## ACT 3: Computing the Bellman update\n","\n","We now perform a single iteration of update to the value function $V$, given state $s$, and discount factor $\\gamma$.\n","You may call a function you have implemented above, and output the difference between the new, updated value for this state, and the previous value (denoted as `delta` in the pseudo-code above)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_TADCWSLfXB4","executionInfo":{"status":"ok","timestamp":1733101770750,"user_tz":300,"elapsed":2,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# ACT 3: update the Value function for state s, that is: V[s], by taking action which maximizes current value. Given:\n","        # Value function V (an array of size num_states),\n","        # State s, and discount factor gamma.\n","# V should be updated in this ACT\n","\n","def bellman_optimality_update(V, s, gamma):\n","    pi = np.zeros((num_states, num_actions))\n","    ### YOUR CODE GOES HERE\n","    old_value = V[s]\n","        # ACT3a: call a function implemented above, to get the action which maximizes current value.\n","    pi, max_val = argmax_over_actions(V, pi, s, gamma)\n","        # ACT3b: update value V[s]\n","    V[s] = max_val\n","        # ACT 3c: compute and output delta.\n","    delta = abs(V[s] - old_value)\n","    return delta"]},{"cell_type":"markdown","metadata":{"id":"i2xStxIWQJiM"},"source":["## ACT 4: Initialize V\n","Initialize $V$ such that for all states, $V[s] = -\\frac{M}{1-\\gamma}$, where $M$ is the upper bound on the absolute value of immediate rewards to be filled in."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tMmwrAeWQJiN","executionInfo":{"status":"ok","timestamp":1733101770750,"user_tz":300,"elapsed":2,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# ACT 4: Initialize V\n","      # length_V gives the size of the vector V\n","def init_V(gamma, M, length_V):\n","    ### YOUR CODE GOES HERE\n","    V = [ -M / (1 - gamma)] * length_V\n","\n","    return V"]},{"cell_type":"markdown","metadata":{"id":"4gXAhs3cQJiN"},"source":["## ACT 5-6: Value-iteration (putting it all together)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LsGJ0nSieiix","executionInfo":{"status":"ok","timestamp":1733101770912,"user_tz":300,"elapsed":164,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["# Now, let's put it all together. Recall that we are given:\n","        # discount factor gamma, and wanted precision theta.\n","def value_iteration(gamma, theta):\n","    # Initialize V with init_V function\n","    ### YOUR CODE GOES HERE\n","    V = init_V(gamma, 1, num_states)\n","\n","    # ACT 5: construct the main loop,\n","    #        iteratively calling the bellman update,\n","    #        and break when sufficient accuracy was reached.\n","    while True:\n","        delta = 0\n","        ### YOUR CODE GOES HERE\n","        for s in range(num_states):\n","          delta = max(delta, bellman_optimality_update(V, s, gamma))\n","\n","        if delta < theta:\n","          break\n","\n","    pi = np.zeros((num_states, num_actions))\n","    # ACT 6: Extract optimal policy using the\n","    # argmax_over_actions function defined in ACT 2\n","    ### YOUR CODE GOES HERE\n","\n","    for s in range(num_states):\n","      pi, max_val = argmax_over_actions(V, pi, s, gamma)\n","\n","    # output optimal value funtion, optimal policy\n","    return V, pi\n"]},{"cell_type":"markdown","metadata":{"id":"5MvIfGO3QJiN"},"source":["## Helper functions (**no action needed**)\n","The next two functions `show_state` and `pretty_print` are helper functions we provide that print and visualize state information."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"WaSJKClYRpAw","executionInfo":{"status":"ok","timestamp":1733101770912,"user_tz":300,"elapsed":2,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["def show_state(env, trial=0, step=0):\n","    plt.figure(5)\n","    plt.clf()\n","    plt.imshow(env.render(mode='rgb_array'))\n","    plt.title(f\"Trial: {trial} | Step: {step}\")\n","    plt.axis('off')\n","    clear_output(wait=True)\n","    display.display(plt.gcf())"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y2GuhYhn4Gtb","executionInfo":{"status":"ok","timestamp":1733101770912,"user_tz":300,"elapsed":1,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[],"source":["def pretty_print(i_episode, t, done, reward, total_rewards, debug=False):\n","  if debug:\n","    show_state(env, i_episode, t)\n","    if done:\n","        print('')\n","        if reward == 1:\n","            total_rewards += 1\n","            print(\">> Success!! got the goal\")\n","        else:\n","            print(\">> Failed!! fell into a hole\")\n","        time.sleep(3), clear_output(wait=True)\n","    else:\n","      time.sleep(.5)\n","    it = i_episode + 1\n","    print(f\"Avg reward so far = {(total_rewards / it):.2f}\")\n","  else:\n","    if done:\n","      if reward == 1:\n","        total_rewards += 1\n","      it = i_episode + 1\n","      if it%10 == 0:\n","        print(f\"Avg reward obtained in iteration {it} = {(total_rewards / it):.2f}\")\n","  return total_rewards"]},{"cell_type":"markdown","metadata":{"id":"6ZpAonqnQJiN"},"source":["## ACT 7: Call agent, and evaluate results\n","\n","Below is the main loop of our setting: first, we call the `value_iteration` function, and obtain the optimal policy.\n","Then, we run in many episodes (or trials) of the algorithm. In each trial, we start at the starting state \"S\". We then ask the agent what action to go towards. We make a step in the frozen lake with the chosen action (but may not move there because the lake is slippery!), our next state is given after we call `env.step(action)`.\n","\n","*You can expect to get an average reward in the range (0.8,1) upon correct implementation.*\n","\n","**Note**: While running over 100 episodes, set `debug = False` to speed up your execution"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"8nj5sjsk15IT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f45491fd-08c9-4fa6-dd49-9980a7685275","executionInfo":{"status":"ok","timestamp":1733101901450,"user_tz":300,"elapsed":446,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Avg reward obtained in iteration 10 = 1.00\n","Avg reward obtained in iteration 20 = 0.95\n","Avg reward obtained in iteration 30 = 0.97\n","Avg reward obtained in iteration 40 = 0.95\n","Avg reward obtained in iteration 50 = 0.90\n","Avg reward obtained in iteration 60 = 0.87\n","Avg reward obtained in iteration 70 = 0.86\n","Avg reward obtained in iteration 80 = 0.85\n","Avg reward obtained in iteration 90 = 0.84\n","Avg reward obtained in iteration 100 = 0.86\n"]}],"source":["# note that you can first try it out with a random agent, by setting this variable to True.\n","random_agent = False\n","episodes = 100\n","\n","# When `debug` is set to True, you can see visualizations of the agent's actions\n","debug = False\n","\n","if not random_agent:\n","    # We have called the value_iteration function:\n","    V_, pi = value_iteration(gamma, theta)\n","    # note that we don't need the value function V anymore, as we already extracted the optimal policy!\n","\n","total_rewards = 0\n","for i_episode in range(episodes):\n","      state = env.reset()\n","      t = 0\n","      while True:\n","          t+=1\n","          # your agent picks an action:\n","          if random_agent:\n","            action = env.action_space.sample() # totally random action !\n","          else:\n","            # ACT 7: get the best action according to optimal policy `pi`, and current state `state`.\n","            # action = 0\n","            ### YOUR CODE GOES HERE\n","            action = np.argmax(pi[state])\n","\n","          # make a step according to policy\n","          state, reward, terminated, truncated, info = env.step(action)\n","          # the above variables are:\n","                # state (object): agent's observation, current state (new state we've reached after the step we took)\n","                # reward (float) : amount of reward returned after previous action\n","                # done (bool): whether the episode has ended\n","                # terminated (bool): whether a `terminal state` is reached (only True if the state is the Goal or a Hole!)\n","                # truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied (eg. timelimit/agent physically going out of bounds)\n","                # info (dict): contains auxiliary diagnostic information (might be helpful for debugging, though not used in this assignment)\n","\n","          done = truncated or terminated # we are done in either case\n","\n","          # visualization\n","          total_rewards = pretty_print(i_episode, t, done, reward, total_rewards, debug=debug)\n","          if done:\n","            break\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"oZtURZfCQJiN"},"source":["## Conceptual Questions (ACTs 8-10)\n","\n","Notice that in your implementation above, an agent that follows the value-iteration policy performs much better than an agent that follows the random policy (you can run both variations by setting the `random_agent` variable to `True/False`).\n","\n","Please **briefly** answer the following questions (1-2 sentences per question suffices)."]},{"cell_type":"markdown","metadata":{"id":"J0beR-B-QJiO"},"source":["#### **ACT8:** Does the value-iteration policy *always* obtains reward=1? Explain why/why not.\n","#### <span style='color:cyan'>  **Answer: No, the value-iteration policy simply maximizes the expected reward which can approach 1 but the environemnt randomness like slipperiness and maybe having holes surrond an area can make it improbable to reach a goal. Thus the value-iteration policy does not always obtain reward = 1** </span>"]},{"cell_type":"markdown","metadata":{"id":"68gglBTJQJiO"},"source":["#### **ACT9:** Will the random policy *always* obtain reward=0? Explain why/why not.\n","#### <span style='color:cyan'>  **Answer: Similar to the previous question, it is possible that the random policy obtains 1 but will not \"always\". The slipperiness aspect of this specific environment can make it possible to \"randomly\" reach the goal of reward 1** </span>"]},{"cell_type":"markdown","metadata":{"id":"KdSUi0u3QJiO"},"source":["#### **ACT10:** When averaged over many episodes, will the value-iteration policy always obtain a larger reward than the random policy? Explain why/why not (no need to prove it, just state the appropriate claim shown in lecture and breifly explain how it implies this statement).\n","#### <span style='color:cyan'>  **Answer: In lecture 16, the value-iteration policy is proven to converge to the max reward given many episodes. Supported in this example, the random policy's expected reward converges to 0 while the value-iteration policy that in this example approaches 0.8 to 1 which is always greater than 0.** </span>"]},{"cell_type":"markdown","source":["**NOTE**: The usual conversion of colab notebook to PDF may not work due to the helper image we add in the Example cell at the beginning of the notebook. This line needs to be removed before converting."],"metadata":{"id":"UoJihf5aTsmw"}},{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc"],"metadata":{"id":"JmwKxNd0Thsx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733101848143,"user_tz":300,"elapsed":76866,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}},"outputId":"82dda634-c124-439d-a8b2-f670b83e9c54","collapsed":true},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n","  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n","  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n","  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n","  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc-data poppler-data\n","  preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n","  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base texlive-binaries\n","  texlive-fonts-recommended texlive-latex-base texlive-latex-recommended texlive-pictures\n","  texlive-plain-generic tipa xfonts-encodings xfonts-utils\n","Suggested packages:\n","  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n","  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java texlive-luatex\n","  pandoc-citeproc context wkhtmltopdf librsvg2-bin groff ghc nodejs php python libjs-mathjax\n","  libjs-katex citation-style-language-styles poppler-utils ghostscript fonts-japanese-mincho\n","  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n","  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf\n","  | pdf-viewer xzdec texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n","  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n","  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n","  default-jre-headless tipa-doc\n","The following NEW packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n","  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n","  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n","  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n","  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc pandoc-data\n","  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc\n","  ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre texlive texlive-base\n","  texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n","  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n","  xfonts-encodings xfonts-utils\n","0 upgraded, 59 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 202 MB of archives.\n","After this operation, 728 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.10 [752 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.10 [5,031 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.8 [50.1 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n","Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n","Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n","Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n","Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n","Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n","Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.8 [5,113 kB]\n","Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n","Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n","Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n","Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n","Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n","Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n","Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n","Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n","Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n","Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n","Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n","Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n","Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n","Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n","Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n","Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n","Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n","Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive all 2021.20220204-1 [14.3 kB]\n","Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n","Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n","Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n","Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n","Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n","Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n","Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n","Fetched 202 MB in 7s (29.4 MB/s)\n","Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 123630 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Selecting previously unselected package fonts-lato.\n","Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n","Unpacking fonts-lato (2.0-2.1) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n","Unpacking poppler-data (0.4.11-1) ...\n","Selecting previously unselected package tex-common.\n","Preparing to unpack .../03-tex-common_6.17_all.deb ...\n","Unpacking tex-common (6.17) ...\n","Selecting previously unselected package fonts-urw-base35.\n","Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n","Unpacking fonts-urw-base35 (20200910-1) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.10_all.deb ...\n","Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n","Selecting previously unselected package libidn12:amd64.\n","Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n","Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.10_amd64.deb ...\n","Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n","Selecting previously unselected package libkpathsea6:amd64.\n","Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package libwoff1:amd64.\n","Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n","Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n","Selecting previously unselected package dvisvgm.\n","Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n","Unpacking dvisvgm (2.13.1-1) ...\n","Selecting previously unselected package fonts-lmodern.\n","Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking fonts-lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n","Unpacking fonts-noto-mono (20201225-1build1) ...\n","Selecting previously unselected package fonts-texgyre.\n","Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n","Unpacking fonts-texgyre (20180621-3.1) ...\n","Selecting previously unselected package libapache-pom-java.\n","Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n","Unpacking libapache-pom-java (18-1) ...\n","Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n","Preparing to unpack .../17-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n","Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n","Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n","Preparing to unpack .../18-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n","Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n","Selecting previously unselected package libcommons-parent-java.\n","Preparing to unpack .../19-libcommons-parent-java_43-1_all.deb ...\n","Unpacking libcommons-parent-java (43-1) ...\n","Selecting previously unselected package libcommons-logging-java.\n","Preparing to unpack .../20-libcommons-logging-java_1.2-2_all.deb ...\n","Unpacking libcommons-logging-java (1.2-2) ...\n","Selecting previously unselected package libfontenc1:amd64.\n","Preparing to unpack .../21-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libptexenc1:amd64.\n","Preparing to unpack .../22-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package rubygems-integration.\n","Preparing to unpack .../23-rubygems-integration_1.18_all.deb ...\n","Unpacking rubygems-integration (1.18) ...\n","Selecting previously unselected package ruby3.0.\n","Preparing to unpack .../24-ruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n","Unpacking ruby3.0 (3.0.2-7ubuntu2.8) ...\n","Selecting previously unselected package ruby-rubygems.\n","Preparing to unpack .../25-ruby-rubygems_3.3.5-2_all.deb ...\n","Unpacking ruby-rubygems (3.3.5-2) ...\n","Selecting previously unselected package ruby.\n","Preparing to unpack .../26-ruby_1%3a3.0~exp1_amd64.deb ...\n","Unpacking ruby (1:3.0~exp1) ...\n","Selecting previously unselected package rake.\n","Preparing to unpack .../27-rake_13.0.6-2_all.deb ...\n","Unpacking rake (13.0.6-2) ...\n","Selecting previously unselected package ruby-net-telnet.\n","Preparing to unpack .../28-ruby-net-telnet_0.1.1-2_all.deb ...\n","Unpacking ruby-net-telnet (0.1.1-2) ...\n","Selecting previously unselected package ruby-webrick.\n","Preparing to unpack .../29-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n","Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n","Selecting previously unselected package ruby-xmlrpc.\n","Preparing to unpack .../30-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n","Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Selecting previously unselected package libruby3.0:amd64.\n","Preparing to unpack .../31-libruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n","Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n","Selecting previously unselected package libsynctex2:amd64.\n","Preparing to unpack .../32-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package libteckit0:amd64.\n","Preparing to unpack .../33-libteckit0_2.5.11+ds1-1_amd64.deb ...\n","Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n","Selecting previously unselected package libtexlua53:amd64.\n","Preparing to unpack .../34-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package libtexluajit2:amd64.\n","Preparing to unpack .../35-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package libzzip-0-13:amd64.\n","Preparing to unpack .../36-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n","Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../37-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../38-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package lmodern.\n","Preparing to unpack .../39-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package pandoc-data.\n","Preparing to unpack .../40-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n","Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n","Selecting previously unselected package pandoc.\n","Preparing to unpack .../41-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n","Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n","Selecting previously unselected package preview-latex-style.\n","Preparing to unpack .../42-preview-latex-style_12.2-1ubuntu1_all.deb ...\n","Unpacking preview-latex-style (12.2-1ubuntu1) ...\n","Selecting previously unselected package t1utils.\n","Preparing to unpack .../43-t1utils_1.41-4build2_amd64.deb ...\n","Unpacking t1utils (1.41-4build2) ...\n","Selecting previously unselected package teckit.\n","Preparing to unpack .../44-teckit_2.5.11+ds1-1_amd64.deb ...\n","Unpacking teckit (2.5.11+ds1-1) ...\n","Selecting previously unselected package tex-gyre.\n","Preparing to unpack .../45-tex-gyre_20180621-3.1_all.deb ...\n","Unpacking tex-gyre (20180621-3.1) ...\n","Selecting previously unselected package texlive-binaries.\n","Preparing to unpack .../46-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n","Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n","Selecting previously unselected package texlive-base.\n","Preparing to unpack .../47-texlive-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-base (2021.20220204-1) ...\n","Selecting previously unselected package texlive-fonts-recommended.\n","Preparing to unpack .../48-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-base.\n","Preparing to unpack .../49-texlive-latex-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-base (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-recommended.\n","Preparing to unpack .../50-texlive-latex-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive.\n","Preparing to unpack .../51-texlive_2021.20220204-1_all.deb ...\n","Unpacking texlive (2021.20220204-1) ...\n","Selecting previously unselected package libfontbox-java.\n","Preparing to unpack .../52-libfontbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libfontbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package libpdfbox-java.\n","Preparing to unpack .../53-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libpdfbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package texlive-pictures.\n","Preparing to unpack .../54-texlive-pictures_2021.20220204-1_all.deb ...\n","Unpacking texlive-pictures (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-extra.\n","Preparing to unpack .../55-texlive-latex-extra_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-extra (2021.20220204-1) ...\n","Selecting previously unselected package texlive-plain-generic.\n","Preparing to unpack .../56-texlive-plain-generic_2021.20220204-1_all.deb ...\n","Unpacking texlive-plain-generic (2021.20220204-1) ...\n","Selecting previously unselected package tipa.\n","Preparing to unpack .../57-tipa_2%3a1.3-21_all.deb ...\n","Unpacking tipa (2:1.3-21) ...\n","Selecting previously unselected package texlive-xetex.\n","Preparing to unpack .../58-texlive-xetex_2021.20220204-1_all.deb ...\n","Unpacking texlive-xetex (2021.20220204-1) ...\n","Setting up fonts-lato (2.0-2.1) ...\n","Setting up fonts-noto-mono (20201225-1build1) ...\n","Setting up libwoff1:amd64 (1.0.2-1build4) ...\n","Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Setting up libijs-0.35:amd64 (0.35-15build2) ...\n","Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Setting up libfontbox-java (1:1.8.16-2) ...\n","Setting up rubygems-integration (1.18) ...\n","Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Setting up fonts-urw-base35 (20200910-1) ...\n","Setting up poppler-data (0.4.11-1) ...\n","Setting up tex-common (6.17) ...\n","update-language: texlive-base not installed and configured, doing nothing!\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n","Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n","Setting up libapache-pom-java (18-1) ...\n","Setting up ruby-net-telnet (0.1.1-2) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up t1utils (1.41-4build2) ...\n","Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n","Setting up fonts-texgyre (20180621-3.1) ...\n","Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n","Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n","Setting up fonts-lmodern (2.004.5-6.1) ...\n","Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n","Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n","Setting up teckit (2.5.11+ds1-1) ...\n","Setting up libpdfbox-java (1:1.8.16-2) ...\n","Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n","Setting up preview-latex-style (12.2-1ubuntu1) ...\n","Setting up libcommons-parent-java (43-1) ...\n","Setting up dvisvgm (2.13.1-1) ...\n","Setting up libcommons-logging-java (1.2-2) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n","Setting up pandoc (2.9.2.1-3ubuntu2) ...\n","Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n","update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n","update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n","Setting up lmodern (2.004.5-6.1) ...\n","Setting up texlive-base (2021.20220204-1) ...\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n","mktexlsr: Updating /var/lib/texmf/ls-R... \n","mktexlsr: Done.\n","tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n","tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n","tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n","tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n","Setting up tex-gyre (20180621-3.1) ...\n","Setting up texlive-plain-generic (2021.20220204-1) ...\n","Setting up texlive-latex-base (2021.20220204-1) ...\n","Setting up texlive-latex-recommended (2021.20220204-1) ...\n","Setting up texlive-pictures (2021.20220204-1) ...\n","Setting up texlive-fonts-recommended (2021.20220204-1) ...\n","Setting up tipa (2:1.3-21) ...\n","Setting up texlive (2021.20220204-1) ...\n","Setting up texlive-latex-extra (2021.20220204-1) ...\n","Setting up texlive-xetex (2021.20220204-1) ...\n","Setting up rake (13.0.6-2) ...\n","Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n","Setting up ruby3.0 (3.0.2-7ubuntu2.8) ...\n","Setting up ruby (1:3.0~exp1) ...\n","Setting up ruby-rubygems (3.3.5-2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","Processing triggers for tex-common (6.17) ...\n","Running updmap-sys. This may take some time... done.\n","Running mktexlsr /var/lib/texmf ... done.\n","Building format(s) --all.\n","\tThis may take some time... done.\n"]}]},{"cell_type":"code","source":["!jupyter nbconvert --to pdf pa5.ipynb"],"metadata":{"id":"D3jEs8YNT84D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733101972316,"user_tz":300,"elapsed":9291,"user":{"displayName":"Andrew Cho","userId":"16926397613903592696"}},"outputId":"a175b836-96fa-4070-8c1b-8d84923fa481"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook pa5.ipynb to pdf\n","[NbConvertApp] Writing 95606 bytes to notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 117170 bytes to pa5.pdf\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}